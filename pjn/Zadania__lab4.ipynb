{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "---\n",
    "Reprezentacja Bag of words (BOW) jest bardzo użyteczna, jednak nie jest pozbawiona wad. Jest ona zazwyczaj pamięciożerna (długość wektora o rozmiarach liczności całego słownika). Co prawda możemy ograniczyć zużycie pamięci poprzez ograniczenie słownika do najważniejszych słów w korpusie, jednak wtedy tracimy informacje, które niosą usunięte słowa, gdyż są one ignorowane. <br/> \n",
    "Innym problemem z tą reprezentacją jest jej niemożność zakodowania informacji o podobieńśtwie słów (co uwidoczni zadanie 1). <br/> Istnieje zatem potrzeba szukania alternatywnych reprezentacji, które pozwolą obejść wyżej wymienione problemy. Jedną z nich są Word Embeddings, których dotyczyć będą dzisiejsze laboratoria.\n",
    "\n",
    "<span style=\"color:red\">Do wykonania zadań potrzebować będziemy zewnętrznych zasobów (embeddingów). Wejdź pod adres: http://nlp.stanford.edu/data/glove.6B.zip, rozpakuj paczkę, a następnie przenieś plik: \"glove.6B.50d.txt\" do folderu, w którym znajudje się ten notebook.</span>\n",
    "\n",
    "# Zadanie 1: Podobieństwo dokumentów - miara cosinusowa\n",
    "Dość częstą potrzebą jest ocena podobieństwa dwóch dokumentów. Kiedy reprezentujemy dokumenty jako wektory równej długości (a używając BOW mamy równą długość wektorów), możemy użyć miary cosinusowej do oceny podobieństwa wektorów.\n",
    "\n",
    "$similarity = cos(\\vec{a}, \\vec{b}) = \\frac{\\sum_{i=1}^na_i b_i}{\\sqrt{\\sum_{i=1}^{n}a_i^2}\\sqrt{\\sum_{i=1}^{n}b_i^2}}$\n",
    "<br/>\n",
    "gdzie $\\vec{a}$ to wektor związany z dokumentem nr 1, a $\\vec{b}$ analogicznie - z dokumentem nr 2.\n",
    "<br/>\n",
    "**Zad1 (1 punkt)** Zaimplementuj miarę podobieństwa cosinusowego między dwoma wektorami dowolnej długości zgodnie z podanym powyżej wzorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np            \n",
    "def cosine(vec1, vec2):\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    \n",
    "    return sum(vec1*vec2)/(np.sqrt(sum(vec1**2))*np.sqrt(sum(vec2**2)))\n",
    "\n",
    "print(cosine([1.0, 2.0, 3.0], [1.5, -0.7, -20]))\n",
    "print(cosine([-10.0, 17.0, 2.0], [5.3, 12.0, -20]))\n",
    "print(cosine([1.0, 2.0, 3.0], [1, -3000, 184]))\n",
    "print(cosine([1.0, 2.0, 3.0], [1, 2, 3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "**Oczekiwany rezultat:** <br/>\n",
    "<ul>\n",
    "<li>-0.797719891818</li>\n",
    "<li>0.234096286977</li>\n",
    "<li>-0.484347153336</li>\n",
    "<li>1.0</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za pomocą CountVectorizera stwórzmy reprezentację BOW dwóch krótkich dokumentów i sprawdźmy z użyciem stworzonej funkcji jakie jest ich podobieństwo. Uruchom poniższy kod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "doc1 = \"Ala ma kota\"\n",
    "doc2 = \"Ala ma pięknego puszystego kota\"\n",
    "\n",
    "docs = [doc1, doc2]\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(docs).todense()\n",
    "\n",
    "print(\"Dokumenty reprezentowane jako BOW. Dokumenty w wierszach, słowa w kolumnach\")\n",
    "print(X_train_counts)\n",
    "print(\"\\n\\nPodobieńśtwo dokumentów to\")\n",
    "print(cosine(X_train_counts[0].tolist()[0], X_train_counts[1].tolist()[0])) # tolist()[0] zamienia macierz 1xn na listę elementów 1xn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kiedy dokumenty zawierają te same słowa podobieństwo zostaje całkiem nieźle obliczone, co jednak, jeśli zamiast takich samych słów znajdziemy wyrazy bliskoznaczne? Sprawdźmy - uruchom poniższy kod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "doc1 = \"kot\"\n",
    "doc2 = \"kotek\"\n",
    "docs = [doc1, doc2]\n",
    "\n",
    "\n",
    "print(\"Dokumenty reprezentowane jako BOW. Dokumenty w wierszach, słowa w kolumnach\")\n",
    "X_train_counts = count_vect.fit_transform(docs).todense()\n",
    "print(X_train_counts)\n",
    "\n",
    "print(\"\\n\\nPodobieńśtwo dokumentów to\")\n",
    "print(cosine(X_train_counts[0].tolist()[0], X_train_counts[1].tolist()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Jak widzimy BOW nie radzi sobie z oceną podobieństwa synonimów. Dajmy zatem szansę embeddingom!\n",
    "\n",
    "---\n",
    "Embeddingi to nic innego jak osadzenie znaczenia słowa w n-wymiarowej przestrzeni tak, aby słowa podobne do siebie występowały blisko w tej n-wymiarowej przestrzeni. Możemy stworzyć je sami na podstawie dużego korpusu tekstu (co może być czasochłonne), z użyciem paczek takich jak: gensim (https://radimrehurek.com/gensim/) ale możemy również użyć \"pretrenowanych\" wektorów utworzonych już na jakimś korpusie, dostępnych np. pod adresem: (https://nlp.stanford.edu/projects/glove/). My wybierzemy opcję drugą - wykorzystania istniejących wektorów. <br/>\n",
    "\n",
    "Embeddingi dostarczone przez zespół Stanforda to pliki tekstowe postaci:<br/>\n",
    "słowo[SPACJA]wektor_liczb_oddzielonych_spacją_reprezentujących_znaczenie_slowa<br/>\n",
    "\n",
    "Funkcja ładująca embeddingi do pamięci została już stworzona. <br/>**Uruchom poniższy kod, aby móc wykrzystać tę funkcję w kolejnych komórkach i ocenić podobieństwo wyrazów bliskoznacznych.** <span style=\"color:red\">Uwaga: zmienna mapping bedzie wykorzystywana w kolejnych komorkach, więc aby była dla nich widoczna - poniższa komórka musi być koniecznie uruchomiona.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_embeddings(path):\n",
    "    mapping = dict()\n",
    "    \n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            splitted = line.split(\" \")\n",
    "            mapping[splitted[0]] = np.array(splitted[1:], dtype=float) # stwórz słownik słowo -> wektor \n",
    "    return mapping\n",
    "\n",
    "mapping = load_embeddings('glove.6B.50d.txt') # ładujemy wektory o długości 50 liczb\n",
    "\n",
    "kot = mapping['cat']\n",
    "kotek = mapping['kitten']\n",
    "krzeslo = mapping['chair']\n",
    "\n",
    "print(\"Podobieńśtwo między kotem a kotkiem:\")\n",
    "print(cosine(kot, kotek))\n",
    "\n",
    "print(\"Podobieńśtwo między kotem a krzesłem\")\n",
    "print(cosine(kot, krzeslo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że embeddingi, które tutaj są dość krótkie (50 liczb opisujących każde słowo) dobrze oddają podobieństwo. Kot jest dużo bardziej podobny do kotka niż do krzesła!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2: Embeddingi - wizualizacja i podobieństwo\n",
    "\n",
    "Embeddingi są reprezentacją znaczenia słów w n-wymiarowej przestrzeni. Strona: http://projector.tensorflow.org próbuje zwizualizować tę przestrzeń poprzez rzutowanie pretrenowanych wektorów na przestrzeń 3-wymiarową.\n",
    "\n",
    "W celu wykonania zadania, otwórz powyższą stronę i wykonaj podpunkty a) i b)  <br/>\n",
    "**Zadanie 2a (0.5 punktu)**: Podaj 5 najbliższych słów do słowa \"data\" w domyślnie załadowanych embeddingach (Słowo, którego sąsiadów chcemy zlokalizować możemy wpisać w pole \"Search\" w górnej części po prawej stronie ekranu. Następnie, po wskazaniu słowa \"data\" ujrzymy listę najbardziej podobnych słów wraz z miarą podobieńtstwa. Listę najbardziej podobnych słów wraz z miarą odległości (wybierz cosinusową), zawrzyj w poniższym komentarzu.\n",
    "<br/>\n",
    "Uwaga - na stronie użyto odległości cosinusowej zamiast podobieństwa. Relacja pomiędzy obiema miarami jest bardzo prosta: odległość = 1 - podobieństwo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Słowo: information  Odległość: 0,435\n",
    "# 2. Słowo: instructions Odległość: 0,506\n",
    "# 3. Słowo: files        Odległość: 0,522\n",
    "# 4. Słowo: file         Odległość: 0,542\n",
    "# 5. Słowo: register     Odległość: 0,547"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widzimy, słowa, które są najbliższe słowu \"data\" to w tym przypadku wyrazy bliskoznaczne. <br/>\n",
    "**Zadanie 2b (0.5 punktu)**: Podaj 5 najbliższych słów do słowa \"red\". Czy nadal mamy do czynienia z synonimami?\n",
    "Odpowiedz na pytanie: jak można interpretować najbardziej podobne słowa(w jakim aspekcie są one podobne), skoro jak widać nie są to synonimy (przypomnij sobie zasadę działania embeddingów)? Odpowiedzi zawrzyj w komentarzach poniżej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Słowo: blue       Odległość: 0,333\n",
    "# 2. Słowo: yellow     Odległość: 0,380\n",
    "# 3. Słowo: white      Odległość: 0,391\n",
    "# 4. Słowo: green      Odległość: 0,396\n",
    "# 5. Słowo: black      Odległość: 0,489\n",
    "\n",
    "# Interpretacja najbliższych słów:\n",
    "Te wyrazy pojawiają się w bardzo podobnym kontekście (wszystkie są przymiotnikami określającymi kolor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 3: Relacje między wektorami \n",
    "\n",
    "Embeddingi zawierają w sobie informacje o znaczeniu. Co więcej - są wektorami, przez co możemy wykonywać na nich operacje (dodawanie, odejmowanie,...). Sprawdźmy jakie efekty uzyskamy wykonując operacje na tych wektorach.\n",
    "<br/>\n",
    "Interesować nas będzie efekt operacji: $\\vec{italy} - \\vec{rome} + \\vec{warsaw}$. Na co wskazywać będzie tak zdefiniowany wektor?<br/>\n",
    "Ponieważ wynikiem tej operacji będzie nowy wektor, napiszmy funkcję, która sprawdzi jakie istniejące słowo leży najbliżej tego wektora.\n",
    "\n",
    "**Zadanie 3:** Wypełnij funkcję get_most_similar() tak, aby dla zadanego wektora vec1, zwróciła słowo, którego wektor jest najbardziej podobny do wektora vec1 (do oceny podobieńśtwa wykorzystaj stworzoną w zad. 1 funkcję cosine). Parametrem embeddings jest słownik, który mapuje słowo na odpowiadający mu wektor.<br/>\n",
    "Jakie słowo zostało wyznaczone jako najbliższe do obliczonego punktu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_similar(vec1, embeddings):\n",
    "    closest = 0\n",
    "    closest_word = \"\"\n",
    "\n",
    "    for word in embeddings.keys():\n",
    "        alike = cosine(vec1, embeddings[word])\n",
    "        if alike > closest:\n",
    "            closest, closest_word = alike, word\n",
    "\n",
    "    return closest_word\n",
    "\n",
    "new_point = mapping['italy'] - mapping['rome'] + mapping['warsaw']\n",
    "print(get_most_similar(new_point, mapping))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy zatem, że wykonywanie operacji na embeddingach pozwala na uzyskanie bardzo ciekawych rezulatów. Jeśli od obiektu \"Włochy\" odejmiemy jego stolicę, a dodamy stolicę polski, to otrzymamy obiekt \"Polska\". Innymi słowy - odpowiadamy na pytanie: jakie słowo jest w takiej relacji do Polski, w jakiej jest Rzym do Włoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 4: Klasyfikacja\n",
    "\n",
    "Okazuje się również, że embeddingi użyteczne są w klasyfikacji, skutecznie redukując nam liczbę cech a także rozwiązując problem rzadkiej reprezentacji. Wróćmy do zadania klasyfikacji spamu znanego z jednych z poprzenich laboratoriów. W celu decydowania czy mamy do czynienia ze spamem, czy może hamem, chcielibyśmy użyć klasyfikatora SVC, który jako cechy przyjmie embeddingi. <br/>\n",
    "\n",
    "Ponieważ jednak embeddingi opisują pojedyncze słowa jako n-wymiarowe wektory, a w problemie klasyfikacji maili musimy reprezentować całe dokumenty w postaci wektorów - musimy zagregować informacje o wszystkich słowach w jednym wektorze cech.\n",
    "<br/>\n",
    "\n",
    "Jedną z zadziwiająco dobrze działających metod okazuje się być reprezentowanie całego dokumentu jako wektora, będącego \"środkiem ciężkości\" słów z których jest zbudowany. Wektor wynikowy jest wektorem n-wymiarowym (tak jak wektor każdego ze słów \"składowych\"), gdzie $i$-ta pozycja wektora posiada wartość będącą średnią arytmetyczną $i$-tych pozycji wektorów słów z danego dokumentu. UWAGA: może okazać się, że w pretrenowanych embeddingach słowo z dokumentu, który chcemy reprezentować jako wektor nie występuje. W takiej syturacji zignorujmy kompletnie to słowo (uznajmy, że go nie ma).\n",
    "<br/>\n",
    "<br/>\n",
    "**Zadanie 4 (1 punkt)**: Zaimplementuj funkcję documents_to_ave_embeddings(docs, embeddings) [ linia 21 ] przyjmującej dwa parametry: \n",
    "<ol>\n",
    "    <li>docs - lista dokumentów w formie tekstowej (lista stringów) do przetransformowania na wektory</li>\n",
    "    <li>embeddings - mapowanie słowo -> wektor (embedding) z istniejącego modelu</li>\n",
    "</ol>\n",
    "Funkcja powinna zwrócić jedną zmienną, listę wektorów dokumentów, gdzie 1 dokument jest \"średnim wektorem\" wektorów słów z których jest zbudowany (jak w paragrafie powyżej treści zadania). Użyj tokenizatora z NLTK (word_tokenize) i przed stokenizowaniem - zamień teksty dokumentów na składające się z samych małych liter.\n",
    "\n",
    "Rozważ użycie numpy.mean z odpowiednią wartością parametru axis do policzenia średniej (będzie łatwiej użyć gotowej funkcji niż ręcznie implementować)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "# ------------------- WCZYTANIE DANYCH -----------\n",
    "\n",
    "full_dataset = pandas.read_csv('data/spam_emails.csv', encoding='utf-8')      # wczytaj dane z pliku CSV\n",
    "full_dataset['label_num'] = full_dataset.label.map({'ham':0, 'spam':1})  # ponieważ nazwy kategorii zapisane są z użyciem stringów: \"ham\"/\"spam\", wykonujemy mapowanie tych wartości na liczby, aby móc wykonać klasyfikację. \n",
    "\n",
    "np.random.seed(0)                                       # ustaw seed na 0, aby zapewnić powtarzalność eksperymentu\n",
    "train_indices = np.random.rand(len(full_dataset)) < 0.7 # wylosuj 70% wierszy, które znajdą się w zbiorze treningowym\n",
    "\n",
    "train = full_dataset[train_indices] # wybierz zbior treningowy (70%)\n",
    "test = full_dataset[~train_indices] # wybierz zbiór testowy (dopełnienie treningowego - 30%)\n",
    "\n",
    "\n",
    "def documents_to_ave_embeddings(docs, embeddings):\n",
    "    result = []\n",
    "    for doc in docs:\n",
    "        words = word_tokenize(doc.lower())\n",
    "        words_embeedings = []\n",
    "        for word in words:\n",
    "            emb = embeddings.get(word)\n",
    "            if emb is not None:\n",
    "                words_embeedings.append(emb)\n",
    "\n",
    "        # words_embeedings = np.array([embeddings.get(word, np.nan) for word in words])\n",
    "        # words_embeedings = words_embeedings[~np.isnan(words_embeedings)]\n",
    "        doc_vector = np.mean(words_embeedings, axis=0)\n",
    "        result.append(doc_vector)\n",
    "    return result\n",
    "        \n",
    "\n",
    "# ------------------- STWORZENIE PIPELINE'U -----------\n",
    " \n",
    "classifier = SVC(C=1.0)\n",
    "\n",
    "train_transformed = documents_to_ave_embeddings(train['text'], mapping)\n",
    "test_transformed = documents_to_ave_embeddings(test['text'], mapping)\n",
    "\n",
    "# ------------------- TRANSFORMACJA I UCZENIE -----------\n",
    "\n",
    "classifier.fit(train_transformed, train['label_num']) # zwektoryzujmy dane i wytrenujmy klasyfikator na zbiorze treningowym\n",
    "\n",
    "# ------------------- OCENA KLASYFIKATORA -----------\n",
    "accuracy = classifier.score(test_transformed, test['label_num'])\n",
    "print(\"W zbiorze testowym {n}% przypadków zostało poprawnie zaklasyfikowanych!\".format(\n",
    "    n=100.*accuracy))\n",
    "print(classification_report(test['label_num'], classifier.predict(test_transformed))) # testowanie klasyfikatora - szerokie podsumowanie uwzględniające miary: precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 5:  Odległość Levensteina (poprawianie literówek)\n",
    "Co prawda odległość Levensteina nie jest związana z koncepcją embeddingów, jednak do tej pory nie próbowaliśmy zastosować tej metody w praktyce. Ostatnie zadanie tych laboratoriów będzie dotyczyło właśnie odległości edycyjnej. \n",
    "\n",
    "**Zadanie 5 (1 punkt)**: Zaimplementuj funkcję get_closest_word która poprawi literówki z użyciem odległości Levensteina. Jeśli aktualny token występuje w słowniku (words_set) - nie powinien zostać korygowany, jeśli zaś nie występuje - powinien być zamieniony na słowo, którego odległość Levensteina jest najmniejsza. Czy udało się poprawić ten zaszumiony tekst?  Wykorzystaj funkcję edit_distance z nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dystans levensteina\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk import word_tokenize\n",
    "from timeit import default_timer as timer              # funkcja potrzebna do mierzenia czasu\n",
    "\n",
    "input_text = \"dzięń dobry pańestwu, labolatolia pżeprowadzone bydą w sali omputerowej 1.6.10. pozdrawiem\"\n",
    "\n",
    "with open('data/slowa_min.txt', 'r', encoding='utf8') as f:\n",
    "    valid_words = set([w.strip() for w in f.read().split(\"\\n\") if w.strip() != '']) # zbiór słów, które są poprawnymi\n",
    "\n",
    "def get_closest_word(token, valid_words):\n",
    "    closest = [len(token)+1, \"\"]\n",
    "    for vw in valid_words:\n",
    "        if (dist := edit_distance(token, vw)) < closest[0]:\n",
    "            closest = [dist, vw]\n",
    "    return closest[1]\n",
    "        \n",
    "\n",
    "tokenized_input = word_tokenize(input_text) # tokenizuj\n",
    "start = timer()  # pobierz czas\n",
    "corrected = []   # lista poprawionych tokenów\n",
    "for token in tokenized_input:\n",
    "    if not token.isalpha() or token in valid_words: # jeśli token nie jest słowem lub jest już w słowniku - nie poprawiaj\n",
    "        corrected.append(token)\n",
    "    else: # popraw\n",
    "        corrected.append(get_closest_word(token, valid_words))\n",
    "end = timer()\n",
    "\n",
    "\n",
    "print(\"Tekst oryginalny: {t}\".format(t=input_text))\n",
    "print(\"Tekst poprawiony: {t}\".format(t=\" \".join(corrected)))\n",
    "print(\"Czas poprawiania: {t}.\".format(t=end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trenowanie własnych wektorów\n",
    "Ponieważ trenowanie wektorów na dużym korpusie może być czasochłonne i wymaga dużego korpusu, aby wyłapać odpowiednie konteksty słów, nie wykonywaliśmy go na laboratoriach. Zainteresowanym tworzeniem własnych wektorów polecam artykuł: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/. Który opisuje jak wytrenować embeddingi z użyciem pythona i paczki \"gensim\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
