{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Uczenie maszynowe z użyciem Scikit-learn i Tensorflow\" A. Geron, rozdział 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pobranie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"zestawy danych\", \"mieszkania\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "# pobranie danych\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patrzymy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe() # ignoruje nulle dla liczby sypialni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w total_bedrooms są nulle\n",
    "# median income, median_age są normalizowane\n",
    "# median_house_value zostało obcięte do max 50 000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyodrębnienie zbioru testowego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# klasycznie\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losowanie warstwowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"median_income\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dzieli wartość przez 1,5 w celu ograniczenia liczby kategorii dochodów\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "# Wartości przekraczające wartość 5 zostają przyporządkowane do klasy 5\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podział na podstawie mediany dochodów (ważnej cechy (wiemy z wiedzy dziedzinowej))\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index] # wybranie wierszy po indeksach\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzenie proporcji przynależności do klas ze wzgl. na medianę dochodów w danych, losowym i lepiej przygotowanym zbiorze testowym\n",
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Łącznie\": income_cat_proportions(housing),\n",
    "    \"L. warstwowe\": income_cat_proportions(strat_test_set),\n",
    "    \"Losowe\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Błąd - losowe (%)\"] = 100 * compare_props[\"Losowe\"] / compare_props[\"Łącznie\"] - 100\n",
    "compare_props[\"Błąd - l. warstwowe (%)\"] = 100 * compare_props[\"L. warstwowe\"] / compare_props[\"Łącznie\"] - 100\n",
    "\n",
    "compare_props\n",
    "# dla warstwowego super, dla losowego gorzej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usunięcie niepotrzebnej kolumny\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do testowych nie wolno nam już zaglądać"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odkrywanie i wizualizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Położenie punktów na mapie (Kalifornia)\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7), \n",
    "             alpha=0.4,\n",
    "             s=housing[\"population\"]/100, label = \"Populacja\",\n",
    "             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "             sharex = False\n",
    "            ) \n",
    "plt.legend()\n",
    "# alpha – przezroczystość każdego punktu\n",
    "# s – rozmiar koła\n",
    "# c – kolor\n",
    "# sharex=False usuwa błąd wyświetlania (nie były wyświetlane wartości osi x i legenda). Jest to tymczasowe rozwiązanie (patrz https://github.com/pandas-dev/pandas/issues/10611). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument `sharex=False` usuwa błąd wyświetlania (nie były wyświetlane wartości osi x i legenda). Jest to tymczasowe rozwiązanie (patrz https://github.com/pandas-dev/pandas/issues/10611). Podziękowania dla Wilmera Arellano za wskazanie tego błędu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.image as mpimg\n",
    "# california_img=mpimg.imread(PROJECT_ROOT_DIR + '/rysunki/pierwszy_projekt/Kalifornia.png')\n",
    "# ax = housing.plot(kind=\"scatter\", x=\"Dł. geograficzna\", y=\"Szer. geograficzna\", figsize=(10,7),\n",
    "#                        s=housing['Populacja']/100, label=\"Populacja\",\n",
    "#                        c=\"Mediana cen mieszkań\", cmap=plt.get_cmap(\"jet\"),\n",
    "#                        colorbar=False, alpha=0.4,\n",
    "#                       )\n",
    "# plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\n",
    "# plt.ylabel(\"Szer. geograficzna\", fontsize=14)\n",
    "# plt.xlabel(\"Dł. geograficzna\", fontsize=14)\n",
    "\n",
    "# prices = housing[\"Mediana cen mieszkań\"]\n",
    "# tick_values = np.linspace(prices.min(), prices.max(), 11)\n",
    "# cbar = plt.colorbar()\n",
    "# cbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\n",
    "# cbar.set_label('Mediana cen mieszkań', fontsize=16)\n",
    "\n",
    "# plt.legend(fontsize=16)\n",
    "# save_fig(\"wykres_punktowy_cen_mieszkań_z_mapą_Kalifornii\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# korelacja pearsona każdego atrybutu z medaną cen\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "plt.show()\n",
    "\n",
    "# na przekątnej histogramy atrybutu (domyślnie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)\n",
    "plt.axis([0, 16, 0, 550000])\n",
    "\n",
    "# widać linię odcięcia na 50 000 oraz z linie w 45 000, 35 000, 28 000. Warto by było wyeliminować te anomalie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nowe cechy\n",
    "housing[\"rooms_per_family\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_rooms\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"avg_family_size\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "\n",
    "#bedrooms_per_rooms lepsze od total_bedrooms, rooms_per_family lepsze od total_rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"rooms_per_family\", y=\"median_house_value\",\n",
    "             alpha=0.2)\n",
    "plt.axis([0, 5, 0, 520000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie danych pod algorytmy uczenia maszynowego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # usuwa etykiety w zbiorze uczącym\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brakujące cechy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # I sp. - usunięcie wierszy z wart. pustymi\n",
    "# sample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # II sp. – usunięcie całej kolumny\n",
    "\n",
    "median = housing[\"total_bedrooms\"].median()\n",
    "sample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # III sp. – zastąpienie zerem, średnią lub medianą\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zautomatyzowanie uzupełniania\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usuwamy atrybut tekstowy, ponieważ mediana może być obliczana wyłącznie wobec atrybutów numerycznych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing.drop('ocean_proximity', axis=1)\n",
    "# housing_num = housing.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer.fit(housing_num) # wyliczenie median\n",
    "# X = imputer.transform(housing_num) # podstawienie median pod nulle. Zwraca listę\n",
    "\n",
    "X = imputer.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_ # wyliczone mediany\n",
    "# housing_num.median().values # tak byśmy to policzyli ręcznie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przekształcamy zbiór uczący:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index = list(housing.index.values)) # przekształcenie listy do DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.loc[sample_incomplete_rows.index.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane kategorialne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing['ocean_proximity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_encoded, housing_categories = housing_cat.factorize() # zamiana str na liczby stałoprzecinkowe\n",
    "print(housing_cat_encoded[:10], housing_categories, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie jest to najlepsze rozwiązanie, bo algorytm może liczyć różnice między cechami, które nie mają praktycznego znaczenia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za pomocą klasy `OneHotEncoder` możemy przekształcić każdą wartość kategorialną do postaci wektora binarnego w którym tylko dla odpowiadającej wartości jest 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1)) # zmiana wektora na macierz i zastosowanie oneHotEncoding\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasa `OneHotEncoder` zwraca domyślnie macierz rzadką (żeby oszczędzać pamięć nie przechowując zer), ale w razie potrzeby możemy ją przekształcić do postaci gęstej: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicja klasy CategoricalEncoder, skopiowana z prośby PR #9151.\n",
    "# Uruchom tę komórkę nie próbując rozumieć jej zawartości (na razie).\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Koduje cechy kategorialne w postaci macierzy numerycznej.\n",
    "    Danymi wejściowymi dostarczanymi do tego transformatora powinna być macierz\n",
    "    zawierająca liczby stałoprzecinkowe lub ciągi znaków, symbolizujące\n",
    "    wartości przechowywane przez cechy kategorialne (dyskretne).\n",
    "    Możemy kodować cechy za pomocą schematu \"gorącojedynkowego\" (jeden-z-K)\n",
    "    (``encoding='onehot'``, domyślne rozwiązanie) lub przekształcać je do postaci\n",
    "    liczb porządkowych (``encoding='ordinal'``).\n",
    "    Tego typu kodowanie jest wymagane podczas dostarczania danych kategorialnych do wielu\n",
    "    etymatorów modułu Scikit-Learn, mianowicie w modelach liniowych i maszynach\n",
    "    SVM wykorzystujących standardowe jądra. Więcej informacji znajdziesz w:\n",
    "    :ref:`User Guide <preprocessing_categorical_features>`.\n",
    "    Parametry\n",
    "    ----------\n",
    "    encoding : ciąg znaków, 'onehot', 'onehot-dense' lub 'ordinal'\n",
    "        Rodzaj stosowanego kodowania (domyślna wartość to 'onehot'):\n",
    "        - 'onehot': koduje cechy za pomocą schematu \"gorącojedynkowego\" (jeden-z-K,\n",
    "           bywa również nazywany kodowaniem 'sztucznym'). Zostaje utworzona kolumna\n",
    "           binarna dla każdej kategorii, a zwracana jest macierz rzadka.\n",
    "        - 'onehot-dense': to samo, co wartość 'onehot', ale zwraca macierz gęstą zamiast rzadkiej.\n",
    "        - 'ordinal': koduje cechy w postaci liczb porządkowych. Uzyskujemy w ten sposób \n",
    "          pojedynczą kolumną zawierającą liczby stałoprzecinkowe (od 0 do n_kategorii - 1) \n",
    "          dla każdej cechy.\n",
    "    categories : 'auto' lub lista list/tablic wartości.\n",
    "        Kategorie (niepowtarzalne wartości) na każdą cechę:\n",
    "        - 'auto' : Automatycznie określa kategorie za pomocą danych uczących. \n",
    "        - lista : ``categories[i]`` przechowuje kategorie oczekiwane w i-tej kolumnie.\n",
    "          Przekazane kategorie zostają posortowanie przed zakodowaniem danych\n",
    "          (użyte kategorie można przejrzeć w atrybucie ``categories_``).\n",
    "    dtype : typ liczby, domyślnie np.float64\n",
    "        Wymagany typ wartości wyjściowej.\n",
    "    handle_unknown : 'error' (domyślnie) lub 'ignore'\n",
    "        Za jego pomocą określamy, czy w przypadku obecności nieznanej cechy w czasie\n",
    "        wykonywania transformacji ma być wyświetlany komunikat o błędzie (wartość\n",
    "        domyślna) lub czy ma zostać zignorowana. Po wybraniu wartości 'ignore' \n",
    "        i natrafieniu na nieznaną kategorię w trakcie przekształceń, wygenerowane\n",
    "        kolumny \"gorącojedynkowe\" dla tej cechy będą wypełnione zerami. \n",
    "        Ignorowanie nieznanych kategorii nie jest obsługiwane w parametrze\n",
    "        ``encoding='ordinal'``.\n",
    "    Atrybuty\n",
    "    ----------\n",
    "    categories_ : lista tablic\n",
    "        Kategorie każdej cechy określone podczas uczenia. W przypadku ręcznego \n",
    "        wyznaczania kategorii znajdziemy tu listę posortowanych kategorii\n",
    "        (w kolejności odpowiadającej wynikowi operacji 'transform').\n",
    "    Przykłady\n",
    "    --------\n",
    "    Mając zbiór danych składający się z trzech cech i dwóch próbek pozwalamy koderowi\n",
    "    znaleźć maksymalną wartość każdej cechy i przekształcić dane do postaci\n",
    "    binarnego kodowania \"gorącojedynkowego\".\n",
    "    >>> from sklearn.preprocessing import CategoricalEncoder\n",
    "    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n",
    "    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    "    ... # doctest: +ELLIPSIS\n",
    "    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n",
    "              encoding='onehot', handle_unknown='ignore')\n",
    "    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n",
    "    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
    "           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
    "    Powiązane materiały\n",
    "    --------\n",
    "    sklearn.preprocessing.OneHotEncoder : przeprowadzana kodowanie \"gorącojedynkowe\"\n",
    "      stałoprzecinkowych cech porządkowych. Klasa ``OneHotEncoder zakłada``, że cechy wejściowe\n",
    "      przechowują wartości w zakresie ``[0, max(cecha)]`` zamiast korzystać z\n",
    "      niepowtarzalnych wartości.\n",
    "    sklearn.feature_extraction.DictVectorizer : przeprowadzana kodowanie \"gorącojedynkowe\"\n",
    "      elementów słowanika (a także cech przechowujących ciągi znaków).\n",
    "    sklearn.feature_extraction.FeatureHasher : przeprowadzana przybliżone kodowanie \"gorącojedynkowe\"\n",
    "      elementów słownika lub ciągów znaków.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Dopasowuje klasę CategoricalEncoder do danych wejściowych X.\n",
    "        Parametry\n",
    "        ----------\n",
    "        X : tablicopodobny, postać [n_próbek, n_cech]\n",
    "            Dane służące do określania kategorii każdej cechy.\n",
    "        Zwraca\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"Należy wybrać jedno z następujących kodowań: 'onehot', 'onehot-dense' \"\n",
    "                        \"lub 'ordinal', wybrano %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"Należy wybrać jedną z następujących wartości parametru handle_unknown: 'error' lub \"\n",
    "                        \"'ignore', wybrano %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"Wartość handle_unknown='ignore' nie jest obsługiwana przez parametr\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Znaleziono nieznane kategorie {0} w kolumnie {1}\"\n",
    "                               \" podczas dopasowywania\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Przekształca X za pomocą kodowania \"gorącojedynkowego\".\n",
    "        Parametry\n",
    "        ----------\n",
    "        X : tablicopodobny, postać [n_próbek, n_cech]\n",
    "            Kodowane dane.\n",
    "        Zwraca\n",
    "        -------\n",
    "        X_out : macierz rzadka lub dwuwymiarowa tablica\n",
    "            Przekształcone dane wejściowe.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csc', dtype=object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=int)\n",
    "        X_mask = np.ones_like(X, dtype=bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Znaleziono nieznane kategorie {0} w kolumnie {1}\"\n",
    "                           \" podczas przekształcania\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Wyznaczamy akceptowalną wartość rzędom sprawiającym problem i\n",
    "                    # kontynuujemy. Rzędy te zostają oznaczone jako `X_mask` i zostaną\n",
    "                    # później usunięte.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasa `CategoricalEncoder` oczekuje dwuwymiarowej tablicy zawierającej co najmniej jedną kategorialną cechę wejściową. Musimy przekształcić dane `housing_cat` do postaci duwywmiarowej tablicy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import CategoricalEncoder # w przyszłych wersjach modułu Scikit-Learn\n",
    "\n",
    "cat_encoder = CategoricalEncoder()\n",
    "housing_cat_reshaped = housing_cat.values.reshape(-1, 1)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domyślne jest stosowane kodowanie \"gorącojedynkowe\", które zwraca macierz rzadką. Możemy ją przekształcić do formy gęstej przy użyciu metody `toarray()`. Ewentualnie możemy wyznaczyć parametr kodowania `\"onehot-dense\"`, aby od razu uzyskać macierz gęstą:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = CategoricalEncoder(encoding=\"onehot-dense\")\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Niestandardowy transformator wstawiający dodatkowe atrybuty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Indeks kolumny\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    # transformerMixin daje metodę fit_transform\n",
    "    # base estimator – get params i set_params (o ile nie używamu args i kwargs)\n",
    "    def __init__(self, add_bedrooms_per_room = True): # żadnych zmiennych *args ani **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nie robi nic innego\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_family = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_family = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_family, population_per_family,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_family, population_per_family]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_extra_attribs = pd.DataFrame(housing_extra_attribs, columns=list(housing.columns)+[\"rooms_per_family\", \"population_per_family\"])\n",
    "housing_extra_attribs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwórzmy teraz potok służący do wstępnej obróbki atrybutów numerycznych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),  # wypełnienie wartości pustych\n",
    "        ('attribs_adder', CombinedAttributesAdder()), # dodanie nowych atrybutów\n",
    "        ('std_scaler', StandardScaler()), # standaryzacja – odjęcie średniej i podzielenie przez wariancję\n",
    "        # nie ma normalizacji (skalowania min-max) do przedziału 0-1\n",
    "    ])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A także transformator służący jedynie do wybierania podzbioru kolumn obiektu DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Tworzy klasę wybierającą numeryczne i kategorialne kolumny,\n",
    "# gdyż moduł Scikit-Learn nie zawiera jeszcze obsługi obiektów DataFrame\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Połączmy teraz te składowe w jeden duży potok przetwarzający wstępnie zarówno cechy numeryczne, jak i kategorialne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)), # wybranie atrybutów numerycznych\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),  # wypełnienie wartości pustych\n",
    "        ('attribs_adder', CombinedAttributesAdder()), # dodanie nowych atrybutów\n",
    "        ('std_scaler', StandardScaler()), # standaryzacja – odjęcie średniej i podzielenie przez wariancję\n",
    "        # nie ma normalizacji (skalowania min-max) do przedziału 0-1\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)), # wybranie atrybutów kategorialnych\n",
    "        ('cat_encoder', CategoricalEncoder(encoding=\"onehot-dense\")), # one hot encoding z wynikiem w postaci gęstej macierzy\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# wykonanie równolegle 2 potoków\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "# użycie samej metody fit() spowodowałoby wywołanie fit_transform na wszystkich elementach potoku oprócz ostatniego\n",
    "# i przekazanie wyniku dalej, i użycie fit() na ostatnim estymatorze.\n",
    "\n",
    "housing_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dobór i uczenie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGRESJA LINIOWA\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "\n",
    "lin_mae = mean_absolute_error(housing_labels, housing_predictions)\n",
    "\n",
    "print(\"rmse:\", lin_rmse, \"\\nmae:\", lin_mae)\n",
    "\n",
    "# To duży błąd patrząc na zakres wartości cen domów (Q1=120 000  Q3=265 000). Model jest niedotrenowany – za prosty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRZEWO DECYZYJNE\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "\n",
    "tree_mae = mean_absolute_error(housing_labels, housing_predictions)\n",
    "\n",
    "print(\"rmse:\", tree_rmse, \"\\nmae:\", tree_mae)\n",
    "\n",
    "# Podejrzane. Przetrenowanie?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regulowanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores) # scores to przeciwieństwo mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Wyniki:\", scores) # 10 wyników dla 10 testów\n",
    "    print(\"Średnia:\", scores.mean())\n",
    "    print(\"Odchylenie standardowe:\", scores.std())\n",
    "\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)\n",
    "\n",
    "# błąd na zbiorze testowym dla drzew decyzyjnych gorszy niż dla regresji liniowej. Przetrenowaliśmy model :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAS LOSOWY – wiele uśrednionych wyników drzew\n",
    "# długo się trenuje\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "forest_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)\n",
    "\n",
    "# błąd na zbiorze walidacyjnym wciąż sporo wyższy niż na uczącym, trochę przetrenowany jeszcze jest – trzeba dobrać parametry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vectore machines\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_reg = SVR(kernel=\"linear\")\n",
    "svm_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions = svm_reg.predict(housing_prepared)\n",
    "svm_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "svm_rmse = np.sqrt(svm_mse)\n",
    "svm_rmse\n",
    "\n",
    "# wygląda obiecująco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu szukalibyśmy jeszcze obiecujących modeli, do dostrojenia..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # wypróbowuje 12 (3×4) kombinacji hiperparametrów\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    # następnie wypróbowuje 6 (2×3) kombinacji z wyłączonym parametrem bootstrap (False)\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# przeprowadza proces uczenia wobec pięciu podzbioru, czyli łącznie (12+6)*5=90 przebiegów \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najlepsza znaleziona kombinacja hiperparametrów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyjrzyjmy się wynikowi każdej kombinacji hiperparametrów przetestowanej w trakcie przeszukiwania siatki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=200),\n",
    "        'max_features': randint(low=1, high=8),\n",
    "    }\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
    "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "rnd_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = rnd_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"Pokoje_na_rodzinę\", \"Populacja_na_rodzinę\", \"Sypialnie_na_pokoje\"]\n",
    "cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"Mediana cen mieszkań\", axis=1)\n",
    "y_test = strat_test_set[\"Mediana cen mieszkań\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Materiały dodatkowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pełen potok zawierający etapy przygotowania i prognozowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", full_pipeline),\n",
    "        (\"linear\", LinearRegression())\n",
    "    ])\n",
    "\n",
    "full_pipeline_with_predictor.fit(housing, housing_labels)\n",
    "full_pipeline_with_predictor.predict(some_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zapisanie stanu modelu za pomocą narzędzia joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = full_pipeline_with_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(my_model, \"mój_model.pkl\") # DIFF\n",
    "#...\n",
    "my_model_loaded = joblib.load(\"mój_model.pkl\") # DIFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykładowe rozkłady w module SciPy za pomocą klasy `RandomizedSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import geom, expon\n",
    "geom_distrib=geom(0.5).rvs(10000, random_state=42)\n",
    "expon_distrib=expon(scale=1).rvs(10000, random_state=42)\n",
    "plt.hist(geom_distrib, bins=50)\n",
    "plt.show()\n",
    "plt.hist(expon_distrib, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozwiązania ćwiczeń"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pytanie: Wypróbuj regresor maszyny wektorów nośnych (`sklearn.svm.SVR`) przy użyciu różnych hiperparametrów, takich jak `kernel=\"linear\"` (oraz różnych wartości hiperparametru `C`) lub `kernel=\"rbf\"` (oraz różnych wartości hiperparametrów `C` i `gamma`). Na razie nie przejmuj się tym, że nie wiesz, do czego te hiperparametry służą. Jak się spisuje najlepszy predyktor maszyny wektorów nośnych?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n",
    "        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n",
    "         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n",
    "    ]\n",
    "\n",
    "svm_reg = SVR()\n",
    "grid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najlepszy model uzyskuje następujący wynik (wyliczony za pomocą pięciokrotnego sprawdzianu krzyżowego): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_mse = grid_search.best_score_\n",
    "rmse = np.sqrt(-negative_mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jest to znacznie gorszy rezultat od uzyskanego za pomocą klasy `RandomForestRegressor`. Sprawdźmy, jakie najlepsze hiperparametry zostały znalezione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jądro liniowe wydaje się być lepsze od jądra RBF. Zwróć uwagę, że wartość parametru `C` stanowi maksymalną przetestowaną wartość. W takiej sytuacji zdecydowanie należy uruchomić ponownie przeszukiwanie siatki z większymi wartościami tego hiperparametru (usuwając jednocześnie mniejsze wartości), ponieważ prawdopodobnie któraś z nich będzie jeszcze lepiej dostosowana do modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytanie: Spróbuj zastąpić klasę `GridSearchCV` klasą `RandomizedSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal\n",
    "\n",
    "# na stronie https://docs.scipy.org/doc/scipy-0.19.0/reference/stats.html\n",
    "# znajdziesz dokumentację funkcji `expon()` i `reciprocal()`, a także innych funkcji rozkładu prawdopodobieństwa.\n",
    "\n",
    "# Uwaga: parametr gamma jest ignorowany, gdy korzystamy z jądra \"linear\"\n",
    "param_distribs = {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': reciprocal(20, 200000),\n",
    "        'gamma': expon(scale=1.0),\n",
    "    }\n",
    "\n",
    "svm_reg = SVR()\n",
    "rnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,\n",
    "                                n_iter=50, cv=5, scoring='neg_mean_squared_error',\n",
    "                                verbose=2, n_jobs=4, random_state=42)\n",
    "rnd_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najlepszy model uzyskuje następujący wynik (obliczony za pomocą pięciokrotnego sprawdzianu krzyżowego):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_mse = rnd_search.best_score_\n",
    "rmse = np.sqrt(-negative_mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz uzyskaliśmy rezultat o wiele bardziej zbliżony do wydajności klasy `RandomForestRegressor` (ciągle mu jednak trochę brakuje). Przyjrzyjmy się znalezionym hiperparametrom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tym razem udało się znaleźć dobry zestaw hiperparametrów dla jądra RBF. Przeszukiwanie losowe zazwyczaj jest w stanie wyszukiwać w tym samym czasie lepsze hiperparametry od przeszukiwania siatki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy wykorzystany przez rozkład wykładniczy, wykorzystujący parametr `scale=1.0`. Zwróć uwagę, że niektóre próbki są znacznie większe lub mniejsze od wartości 1.0, ale gdy sprawdzimy logarytm tego rozkładu, zauważymy, że większość wartości mieści się głównie w przedziale od exp(-2) do exp(+2), czyli od mniej więcej 0.1 do 7.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expon_distrib = expon(scale=1.)\n",
    "samples = expon_distrib.rvs(10000, random_state=42)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Rozkład wykładniczy (scale=1.0)\")\n",
    "plt.hist(samples, bins=50)\n",
    "plt.subplot(122)\n",
    "plt.title(\"Logarytm tego rozkładu\")\n",
    "plt.hist(np.log(samples), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozkład użyty dla hiperparametru `C` wygląda całkiem inaczej: skala próbek została wyznaczona z rozkładu jednorodnego o określonym zakresie, co stanowi powód, że wykres widoczny po prawej (reprezentujący logarytm rozkładu) jest mniej więcej jednolity. Rozkład ten przydaje się, gdy nie mamy pojęcia, jaka jest docelowa skala:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reciprocal_distrib = reciprocal(20, 200000)\n",
    "samples = reciprocal_distrib.rvs(10000, random_state=42)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Rozkład odwrotny (scale=1.0)\")\n",
    "plt.hist(samples, bins=50)\n",
    "plt.subplot(122)\n",
    "plt.title(\"Logarytm tego rozkładu\")\n",
    "plt.hist(np.log(samples), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozkład odwrotny przydaje się wtedy, gdy nie wiemy, w jakiej skali powinien znajdować się dany hiperparametr (rzeczywiście, na prawym wykresie widzimy, że wszystkie skale mają mniej więcej takie samo prawdopodobobieństwo w określonym zakresie), natomiast rozkład wykładniczy nadaje się w sytuacjach, gdy w przybliżeniu znamy skalę hiperparametru."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytanie: 3.\tSpróbuj dodać w potoku przygotowawczym funkcję przekształcającą w taki sposób, aby były dobierane wyłącznie najistotniejsze atrybuty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def indices_of_top_k(arr, k):\n",
    "    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
    "\n",
    "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_importances, k):\n",
    "        self.feature_importances = feature_importances\n",
    "        self.k = k\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[:, self.feature_indices_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uwaga: Omawiany selektor cech zakłada, że już w jakiś sposób obliczyliśmy istotności cech (np. za pomocą klasy `RandomForestRegressor`). Być może kusi Cię, aby obliczyć je bezpośrednio w metodzie fit() klasy `TopFeatureSelector`, ale rozwiązanie to prawdopodobnie spowolniłoby proces przeszukiwania siatki/losowego, ponieważ istotności cech muszą zostać wyliczone dla każdej kombinacji hiperparametrów (chyba że zaimplementujesz jakąś formę pamięci podręcznej)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdefiniujmy liczbę najlepszych cech, które chcemy przechować:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyjrzyjmy się teraz indeksom k najlepszych cech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_feature_indices = indices_of_top_k(feature_importances, k)\n",
    "top_k_feature_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(attributes)[top_k_feature_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upewnijmy się, że mamy rzeczywiście do czynienia z k najlepszymi cechami:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(feature_importances, attributes), reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygląda nieźle... Stwórzmy teraz nowy potok, w którym połączymy wcześniej zdefiniowany potok przygotowawczy z doborem k najlepszych cech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_and_feature_selection_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyjrzyjmy się cechom trzech pierwszych próbek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared_top_k_features[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upewnijmy się, że mamy do czynienia z k najlepszymi cechami: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared[0:3, top_k_feature_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wszystko śmiga!  :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytanie: 4.\tSpróbuj stworzyć pojedynczy potok przeprowadzający pełne przygotowywanie danych i generujący ostateczne prognozy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_select_and_predict_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n",
    "    ('svm_reg', SVR(**rnd_search.best_params_))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_select_and_predict_pipeline.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wypróbujmy pełen potok na kilku próbkach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = housing.iloc[:4]\n",
    "some_labels = housing_labels.iloc[:4]\n",
    "\n",
    "print(\"Prognozy:\\t\", prepare_select_and_predict_pipeline.predict(some_data))\n",
    "print(\"Etykiety:\\t\\t\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygląda na to, że pełen potok działa prawidłowo. Prognozy, oczywiście, nie są jakieś wybitne: byłyby lepsze, gdybyśmy użyli najlepszego znalezionego wcześniej regresora `RandomForestRegressor` zamiast `SVR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytanie: 1.\tSprawdź automatycznie niektóre funkcje przygotowawcze za pomocą klasy `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "        {'preparation__num_pipeline__imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "         'feature_selection__k': list(range(1, len(feature_importances) + 1))}\n",
    "]\n",
    "\n",
    "grid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n",
    "                                scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\n",
    "grid_search_prep.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_prep.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najlepszą strategią klasy imputer jest most_frequent i najwidoczniej niemal wszystkie cechy okazują się przydatne (15 z 16). Ostatnia cecha (ISLAND) prawdopodobnie wprowadza jedynie dodatkowy szum.\n",
    "\n",
    "Gratulacje! Wiesz już całkiem dużo na temat uczenia maszynowego. :) You already know quite a lot about Machine Learning. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
