{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD7yUVVRDtiU"
      },
      "source": [
        "# Lab2: Generating embeddings from Recurrent Neural Networks\n",
        "\n",
        "Task 1: Please visit the website: https://allenai.org/allennlp/software/elmo\n",
        "\n",
        "Here a library implementing ELMo is provided. Read the docs on this website, and move to \"Demo\" website. Here you can find some code examples trying to solve a reading comprehension task using ELMo embeddings and a model called BIDAF. \n",
        "\n",
        "Please follow the instruction here to:\n",
        "1. install AllenNLP library\n",
        "2. Instantiate the BIDAF-ELMO model from Python code (please omit the command line examples) and experiment with several examples that you'll come up with.\n",
        "\n",
        "List your examples in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.rc\n",
        "\n",
        "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/bidaf-elmo.2021-02-11.tar.gz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictor.predict(\n",
        "    passage=\"A hand tool, consisting of a weighted head fixed to a long handle that is swung to deliver an impact to a small area of an object. This can be, for example, to drive nails into wood, to shape metal (as with a forge), or to crush rock.\",\n",
        "    question=\"What tool are they talking about?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictor.predict(\n",
        "    passage = 'On October 3, 2023, Sylwester Wardęga published a video titled THE DARK SECRET OF STUU AND YOUTUBERS: PANDORA GATE (Boxdel, Dubiel, Fagata), which contains extensive evidence accusing Stuart Burton of pedophilia and Marcin Dubiel, Michał \"Boxdel\" Baron, Agata \"Fagata\" Fąk of concealing knowledge about a possible crime.'\n",
        "    question = 'Who was accused in the video?'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SifA0N_IHLeS"
      },
      "source": [
        "Task 2: Preparing a language model from scratch using PyTorch\n",
        "\n",
        "We can prepare and train a language model using some existing libraries. One of the most popular ones is PyTorch (https://pytorch.org). There are several tutorials showing how you can use PyTorch to train a language model. I found this one easy to follow (https://towardsdatascience.com/language-modeling-with-lstms-in-pytorch-381a26badcbf). Try to follow this tutorial and prepare your own language model (store the code you use here in the notebook). The authors state that the training may take about two hours on Google Colab. We don't need to train our network for that long. \n",
        "\n",
        "Instead of using 50 epochs (n_epochs = 50), let's limit the epoch number to 10.\n",
        "Of course, more epochs lead to better results.\n",
        "\n",
        "**Important note**: If you can't open the link with the tutorial, open the browser in the incognito mode :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/anna/.local/lib/python3.10/site-packages (from datasets) (1.23.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /home/anna/.conda/envs/ml/lib/python3.10/site-packages (from datasets) (12.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/anna/.conda/envs/ml/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /home/anna/.local/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /home/anna/.conda/envs/ml/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /home/anna/.conda/envs/ml/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /home/anna/.conda/envs/ml/lib/python3.10/site-packages (from datasets) (2023.5.0)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /home/anna/.local/lib/python3.10/site-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/anna/.local/lib/python3.10/site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/anna/.local/lib/python3.10/site-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/anna/.conda/envs/ml/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: filelock in /home/anna/.conda/envs/ml/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/anna/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/anna/.conda/envs/ml/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/anna/.conda/envs/ml/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/anna/.conda/envs/ml/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/anna/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/anna/.local/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /home/anna/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multiprocess, multidict, frozenlist, async-timeout, yarl, huggingface-hub, aiosignal, aiohttp, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.10.1\n",
            "    Uninstalling huggingface-hub-0.10.1:\n",
            "      Successfully uninstalled huggingface-hub-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cached-path 1.1.6 requires huggingface-hub<0.11.0,>=0.8.1, but you have huggingface-hub 0.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.5 frozenlist-1.4.0 huggingface-hub-0.18.0 multidict-6.0.4 multiprocess-0.70.15 xxhash-3.4.1 yarl-1.9.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3Wn-V_5RIWiy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7e604c7170>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import torchtext\n",
        "import datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    test: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 4358\n",
            "    })\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 36718\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3760\n",
            "    })\n",
            "})\n",
            "dict_keys(['text'])\n",
            " This ammunition , and that which I brought with me , was rapidly prepared for use at the Laboratory established at the Little Rock Arsenal for that purpose . As illustrating as the pitiful scarcity of material in the country , the fact may be stated that it was found necessary to use public documents of the State Library for cartridge paper . Gunsmiths were employed or conscripted , tools purchased or impressed , and the repair of the damaged guns I brought with me and about an equal number found at Little Rock commenced at once . But , after inspecting the work and observing the spirit of the men I decided that a garrison 500 strong could hold out against Fitch and that I would lead the remainder - about 1500 - to Gen 'l Rust as soon as shotguns and rifles could be obtained from Little Rock instead of pikes and lances , with which most of them were armed . Two days elapsed before the change could be effected . \" \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# load dataset\n",
        "dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
        "print(dataset)\n",
        "print(dataset['train'][88].keys())\n",
        "print(dataset['train'][88]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c465dd7940746c4839130549cbbb079",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e7c697d25d942639fd6d3072ee68081",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93a6cc015b734ff4a1ee08c460b431b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['this', 'ammunition', ',', 'and', 'that', 'which', 'i', 'brought', 'with', 'me', ',', 'was', 'rapidly', 'prepared', 'for', 'use', 'at', 'the', 'laboratory', 'established', 'at', 'the', 'little', 'rock', 'arsenal', 'for', 'that', 'purpose', '.', 'as', 'illustrating', 'as', 'the', 'pitiful', 'scarcity', 'of', 'material', 'in', 'the', 'country', ',', 'the', 'fact', 'may', 'be', 'stated', 'that', 'it', 'was', 'found', 'necessary', 'to', 'use', 'public', 'documents', 'of', 'the', 'state', 'library', 'for', 'cartridge', 'paper', '.', 'gunsmiths', 'were', 'employed', 'or', 'conscripted', ',', 'tools', 'purchased', 'or', 'impressed', ',', 'and', 'the', 'repair', 'of', 'the', 'damaged', 'guns', 'i', 'brought', 'with', 'me', 'and', 'about', 'an', 'equal', 'number', 'found', 'at', 'little', 'rock', 'commenced', 'at', 'once', '.', 'but', ',', 'after', 'inspecting', 'the', 'work', 'and', 'observing', 'the', 'spirit', 'of', 'the', 'men', 'i', 'decided', 'that', 'a', 'garrison', '500', 'strong', 'could', 'hold', 'out', 'against', 'fitch', 'and', 'that', 'i', 'would', 'lead', 'the', 'remainder', '-', 'about', '1500', '-', 'to', 'gen', \"'\", 'l', 'rust', 'as', 'soon', 'as', 'shotguns', 'and', 'rifles', 'could', 'be', 'obtained', 'from', 'little', 'rock', 'instead', 'of', 'pikes', 'and', 'lances', ',', 'with', 'which', 'most', 'of', 'them', 'were', 'armed', '.', 'two', 'days', 'elapsed', 'before', 'the', 'change', 'could', 'be', 'effected', '.']\n"
          ]
        }
      ],
      "source": [
        "# tokenize data\n",
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
        "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}  \n",
        "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], \n",
        "    fn_kwargs={'tokenizer': tokenizer}\n",
        "    )\n",
        "print(tokenized_dataset['train'][88]['tokens'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29473\n",
            "['<unk>', '<eos>', 'the', ',', '.', 'of', 'and', 'in', 'to', 'a']\n"
          ]
        }
      ],
      "source": [
        "# create vocabulary\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], \n",
        "min_freq=3) \n",
        "vocab.insert_token('<unk>', 0)           \n",
        "vocab.insert_token('<eos>', 1)            \n",
        "vocab.set_default_index(vocab['<unk>'])  # for OOV \n",
        "print(len(vocab))\n",
        "print(vocab.get_itos()[:10]) # index to string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data(dataset, vocab, batch_size):\n",
        "    ### Add eos to the end of each example, convert to indices and reshape data into 2D tensor (batch_size, num_batches) ###\n",
        "    data = []                                                   \n",
        "    for example in dataset:\n",
        "        if example['tokens']:                                      \n",
        "            tokens = example['tokens'].append('<eos>')             \n",
        "            tokens = [vocab[token] for token in example['tokens']] \n",
        "            data.extend(tokens)                                    \n",
        "    data = torch.LongTensor(data)                                 \n",
        "    num_batches = data.shape[0] // batch_size \n",
        "    data = data[:num_batches * batch_size]                       \n",
        "    data = data.view(batch_size, num_batches)          \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Divide data to batches\n",
        "batch_size = 128\n",
        "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
        "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
        "test_data = get_data(tokenized_dataset['test'], vocab, batch_size)\n",
        "# We have 16214 batches, each of 128 words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, \n",
        "                tie_weights):\n",
        "                \n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
        "                    dropout=dropout_rate, batch_first=True) # num layers – how many recurrent layers\n",
        "        self.dropout = nn.Dropout(dropout_rate) # last layer didn't have dropout. Used also for embedding layer\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size) # classification layer\n",
        "        \n",
        "        if tie_weights:\n",
        "            # embedding layer share weights with the output layer. Helps reduce the number of parameters, beacaues output weights also learn word embeddings in some sense\n",
        "            assert embedding_dim == hidden_dim, 'cannot tie, check dims'\n",
        "            self.embedding.weight = self.fc.weight\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, src, hidden):\n",
        "        embedding = self.dropout(self.embedding(src))\n",
        "        output, hidden = self.lstm(embedding, hidden)          \n",
        "        output = self.dropout(output) \n",
        "        prediction = self.fc(output)\n",
        "        return prediction, hidden\n",
        "    \n",
        "    def init_weights(self):\n",
        "        #### init weights for all layers to learn more effectively ###\n",
        "        init_range_emb = 0.1\n",
        "        init_range_other = 1/math.sqrt(self.hidden_dim)\n",
        "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
        "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
        "        self.fc.bias.data.zero_()\n",
        "        for i in range(self.num_layers):\n",
        "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
        "                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
        "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim, \n",
        "                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        ### set the LSTM’s hidden and cell state to zero ###\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return hidden, cell\n",
        "    \n",
        "    def detach_hidden(self, hidden):\n",
        "        ### for independent hiddent states ###\n",
        "        hidden, cell = hidden\n",
        "        hidden = hidden.detach()\n",
        "        cell = cell.detach()\n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 1024             # 400 in the paper\n",
        "hidden_dim = 1024                # 1150 in the paper\n",
        "num_layers = 2                   # 3 in the paper\n",
        "dropout_rate = 0.65              \n",
        "tie_weights = True               # that's why embedding_dim == hidden_dim              \n",
        "lr = 1e-3                        # They used 30 and a different optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 47,003,425 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "# init the model\n",
        "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_batch(data, seq_len, num_batches, idx):\n",
        "    # we're using a fixed backpropagation through time window.\n",
        "    # it’sa way to deal with the problem that we can’t have a batch of sequences with unequal lengths.\n",
        "    # sequences are not equal length, so we take words from multiple sequences and don't reset the hidden state\n",
        "    src = data[:, idx:idx+seq_len]                   \n",
        "    target = data[:, idx+1:idx+seq_len+1] # next words in sequence       \n",
        "    return src, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train() # dropout is not disabled\n",
        "    \n",
        "    # drop all batches that are not a multiple of seq_len\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    \n",
        "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):  # The last batch can't be a src\n",
        "        optimizer.zero_grad()\n",
        "        hidden = model.detach_hidden(hidden)\n",
        "\n",
        "        src, target = get_batch(data, seq_len, num_batches, idx)\n",
        "        src, target = src.to(device), target.to(device)\n",
        "        batch_size = src.shape[0]\n",
        "        prediction, hidden = model(src, hidden)  # [batch size, seq_len, vocab size]\n",
        "\n",
        "        prediction = prediction.reshape(batch_size * seq_len, -1)  # [batch_size*seq_len, vocab]\n",
        "        target = target.reshape(-1) # [batch size * seq_len]\n",
        "        loss = criterion(prediction, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # to sidestep exploding gradient\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * seq_len # the loss is averaged over the token and we want to avage by sequences\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.eval()\n",
        "    \n",
        "    # drop all batches that are not a multiple of seq_len\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, num_batches - 1, seq_len):\n",
        "            hidden = model.detach_hidden(hidden)\n",
        "            src, target = get_batch(data, seq_len, num_batches, idx)\n",
        "            src, target = src.to(device), target.to(device)\n",
        "            batch_size= src.shape[0]\n",
        "\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
        "            target = target.reshape(-1)\n",
        "\n",
        "            loss = criterion(prediction, target)\n",
        "            epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/324 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 863.409\n",
            "\tValid Perplexity: 2837.578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                            \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb Komórka 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m best_valid_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_data, optimizer, criterion, \n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                 batch_size, seq_len, clip, device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model, valid_data, criterion, batch_size, \n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                 seq_len, device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep(valid_loss)\n",
            "\u001b[1;32m/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb Komórka 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m src, target \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m batch_size \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m prediction, hidden \u001b[39m=\u001b[39m model(src, hidden)               \n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m prediction \u001b[39m=\u001b[39m prediction\u001b[39m.\u001b[39mreshape(batch_size \u001b[39m*\u001b[39m seq_len, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)   \n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m~/.conda/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.conda/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;32m/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb Komórka 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src, hidden):\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(src))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(embedding, hidden)          \n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(output) \n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/dane/projekty/moje/ml/natural_language_processing/notebooks_from_dr_Wisniewski/03_contextual_embeddings.ipynb#X33sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(output)\n",
            "File \u001b[0;32m~/.conda/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.conda/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/ml/lib/python3.10/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    880\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    881\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_epochs = 10\n",
        "seq_len = 50\n",
        "clip = 0.25\n",
        "saved = True\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0) # reduce the learning rate after every epoch associated with no improvement\n",
        "\n",
        "if saved:\n",
        "    model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
        "    test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
        "    print(f'Test Perplexity: {math.exp(test_loss):.3f}')\n",
        "else:\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss = train(model, train_data, optimizer, criterion, \n",
        "                    batch_size, seq_len, clip, device)\n",
        "        valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
        "                    seq_len, device)\n",
        "        \n",
        "        lr_scheduler.step(valid_loss)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), 'best-val-lstm_lm.pt') # save the model with the best validation loss\n",
        "\n",
        "        print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
        "        print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    model.eval()\n",
        "    tokens = tokenizer(prompt)\n",
        "    indices = [vocab[t] for t in tokens]\n",
        "    batch_size = 1\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(max_seq_len):\n",
        "            src = torch.LongTensor([indices]).to(device)\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  # higher temperature decreases the “confidence” a model has in its most likely response. https://lukesalamone.github.io/posts/what-is-temperature/\n",
        "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
        "            \n",
        "            while prediction == vocab['<unk>']:\n",
        "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            if prediction == vocab['<eos>']:\n",
        "                break\n",
        "\n",
        "            indices.append(prediction)\n",
        "\n",
        "    itos = vocab.get_itos()\n",
        "    tokens = [itos[i] for i in indices]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = 'Think about'\n",
        "max_seq_len = 30\n",
        "seed = 0\n",
        "\n",
        "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
        "for temperature in temperatures:\n",
        "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
        "                          vocab, device, seed)\n",
        "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
